{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha Worm\n",
    "\n",
    "**About the project**\n",
    "\n",
    "This project is a student project of a group of 4 LMU students from the computer science department. Objective of the project is to implement one (or more) RL approaches to solve a Unity ML Agent ([https://github.com/Unity-Technologies/ml-agents](https://github.com/Unity-Technologies/ml-agents)) domain. Here we consider the **Worm Domain** from Unity.\n",
    "\n",
    "The following notebook shows how to interact with the DDPG algortihm.\n",
    "\n",
    "---\n",
    "\n",
    "**Some notes/issues we faced:**\n",
    "* **You can't use the Windows Subsystem for Linux**!\n",
    "* The executable/environment has to build for the platform where you execute on\n",
    "* You have to install `tensorflow = 1.15.3` (the latest tensorflow version doesn't work)\n",
    "* Make sure that the environments are closed in Python (`env.close()`) after execution! If they are not closed probably the unity window would freeze\n",
    "\n",
    "**About the Pyhton-Unity connection:**\n",
    "* You can use a standalone builded environment (i.e. an .exe for Windows or an .x86_64)\n",
    "* Alternatively, you can also interact through Python with an Unity environment that is open in the the Unity Editor\n",
    "    * Just passing `None` in `env = UnityEnvironment(file_name=None)`\n",
    "    * You have to press play to interact in th environment\n",
    "    * Benefit: Here you have console outputs!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is important to append the parent directory to the Python path to import modules from our package.\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "# For correclty referencing the directoy e.g. when you save files, you have to change the working directoy of this notebook.\n",
    "# Important: If you reimport you have to restart the kernel. Otherwise you would always go one directory above.\n",
    "import os\n",
    "os.chdir(\"../\")\n",
    "\n",
    "####################\n",
    "# Default Packages #\n",
    "####################\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "##################\n",
    "# ML/RL Packages #\n",
    "##################\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "################\n",
    "# Our Packages #\n",
    "################\n",
    "from trainer import DDPGTrainer, TD3Trainer\n",
    "from utils.mlagent_utils import get_env\n",
    "from config.config import log, logFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train: Pendulum Domain (or any other gym environment)\n",
    "\n",
    "You will find the tracked results and any file that is tracked in `models/CURRENT_DATE/NAME/` (starting from the project root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# Set Training Infos #\n",
    "######################\n",
    "env_name = \"Pendulum-v0\"\n",
    "name = f\"DPPG-{env_name}\"\n",
    "\n",
    "###########################################\n",
    "# Ensure that the path exists for logging #\n",
    "###########################################\n",
    "folder = Path(f'models/{datetime.now().date()}/{name}/')\n",
    "folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Store logs directly nearby the results!\n",
    "fh = log.FileHandler(f'models/{datetime.now().date()}/{name}/{datetime.now().date()}.log')\n",
    "fh.setFormatter(logFormatter)\n",
    "log.getLogger().addHandler(fh)\n",
    "\n",
    "######################\n",
    "# Create Environment #\n",
    "######################\n",
    "env = gym.make(env_name)\n",
    "\n",
    "trainer = DDPGTrainer()\n",
    "\n",
    "log.info(f\"Start DDPG training ({env_name})...\")\n",
    "\n",
    "# If you want to customize the training.\n",
    "# trainer.config[\"episodes\"] = 5\n",
    "# trainer.config[\"training_steps\"] = 5\n",
    "# trainer.config[\"evaluation_steps\"] = 5 # To disable evaluation set to 0\n",
    "# trainer.config[\"evaluation_lim\"] = 10\n",
    "\n",
    "trainer.train(env, name=name, render=False)\n",
    "\n",
    "log.info(\"Training done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train: Worm Domain\n",
    "\n",
    "You will find the tracked results and any file that is tracked in `models/CURRENT_DATE/NAME/` (starting from the project root)\n",
    "\n",
    "For plotting the results after training you can use the internal stored results:\n",
    "* E.g. `trainer.training_rewards_df.plot()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# Set Training Infos #\n",
    "######################\n",
    "name = f\"DPPG-AlphaWorm\"\n",
    "\n",
    "###########################################\n",
    "# Ensure that the path exists for logging #\n",
    "###########################################\n",
    "folder = Path(f'models/{datetime.now().date()}/{name}/')\n",
    "folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Store logs directly nearby the results!\n",
    "fh = log.FileHandler(f'models/{datetime.now().date()}/{name}/{datetime.now().date()}.log')\n",
    "fh.setFormatter(logFormatter)\n",
    "log.getLogger().addHandler(fh)\n",
    "\n",
    "env = \"envs/worm_dynamic_one_agent/win/UnityEnvironment\"  # For Windows\n",
    "# env = \"./envs/worm_dynamic_one_agent/linux/worm_dynamic\"  # For Linux\n",
    "env = get_env(env, True)\n",
    "\n",
    "trainer = DDPGTrainer()\n",
    "\n",
    "log.info(\"Start DDPG training (WormDomain)...\")\n",
    "\n",
    "# If you want to customize the training.\n",
    "# trainer.config[\"episodes\"] = 5\n",
    "# trainer.config[\"training_steps\"] = 10\n",
    "# trainer.config[\"evaluation_steps\"] = 1 # To disable evaluation set to 0\n",
    "# trainer.config[\"evaluation_lim\"] = 10\n",
    "# trainer.config[\"explore_threshold\"] = 0.1\n",
    "\n",
    "trainer.train(env, name=name)\n",
    "\n",
    "log.info(\"Training done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## HPO Training: Worm Domain\n",
    "\n",
    "Optuna stores the HPO results and states in a so called `study`. Such a study can be used to evaluate and view the results during HPO. Here are some examples:\n",
    "\n",
    "* `study.best_params`for getting the parameters of the best performing model\n",
    "\n",
    "For a full API see [https://optuna.readthedocs.io/en/stable/](https://optuna.readthedocs.io/en/stable/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Set Training Infos #\n",
    "######################\n",
    "name = f\"DPPG-AlphaWorm-HPO\"\n",
    "\n",
    "###########################################\n",
    "# Ensure that the path exists for logging #\n",
    "###########################################\n",
    "folder = Path(f'models/{datetime.now().date()}/{name}/')\n",
    "folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Store logs directly nearby the results!\n",
    "fh = log.FileHandler(f'models/{datetime.now().date()}/{name}/{datetime.now().date()}.log')\n",
    "fh.setFormatter(logFormatter)\n",
    "log.getLogger().addHandler(fh)\n",
    "\n",
    "env = \"envs/worm_dynamic_one_agent/win/UnityEnvironment\"  # For Windows\n",
    "# env = \"./envs/worm_dynamic_one_agent/linux/worm_dynamic\"  # For Linux\n",
    "env = get_env(env, False)\n",
    "\n",
    "trainer = DDPGTrainer()\n",
    "\n",
    "# Important: Start training is only needed for HPO.\n",
    "# Important: If you set default = True and set the number of trials > 1 you train multiple times on the same parameters!\n",
    "# For using the search space use default = False.\n",
    "study = trainer.start_training(env, trials=2, render=False, name=name, default=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Plotting study results #\n",
    "##########################\n",
    "import optuna\n",
    "# optuna.visualization.plot_optimization_history(study)\n",
    "# optuna.visualization.plot_intermediate_values(study)\n",
    "# optuna.visualization.plot_optimization_history(study)\n",
    "# optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load already stored agents\n",
    "\n",
    "By default agents are dumped using pickle in the model directory. \n",
    "\n",
    "**Important**: Pickle dumps the object in a specific context and we have to make sure that the context matches while we loading the dumped object! See comments below.\n",
    "\n",
    "BTW: This procedure also works, if you want to review a specific study again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: Pickle dumps object in a specific context. Therefore it is very important to also reconstruct this context.\n",
    "# This means imply make sure that you add the parent directoy to the python path.\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "import pickle\n",
    "\n",
    "# Example path! Replace by your path.\n",
    "with open(\"../models/2020-07-03/DDPG-Pendulum-2/ddpg_training.pickle\", \"rb\") as f:\n",
    "    ddpg_agent = pickle.load(f)\n",
    "\n",
    "# Use the trained agent! E.g. by runing on a specific environment.\n",
    "# ddpg_agent.run(env, steps=1000)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitaspconda76abd147041447eca13278d9df449fb2",
   "display_name": "Python 3.7.7 64-bit ('asp': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}