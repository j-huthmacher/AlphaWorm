{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test training method (Worm Domain).\n",
    "\n",
    "@author: j-huthmacher\n",
    "\n",
    "For testing and debugging!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"../\")\n",
    "import os\n",
    "os.chdir(\"../\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "import gym\n",
    "from trainer import DDPGTrainer, TD3Trainer\n",
    "from utils.mlagent_utils import get_env\n",
    "from optuna.trial import FixedTrial\n",
    "from config.config import log\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# win_env = \"envs/worm_dynamic_one_agent/win/UnityEnvironment\"\n",
    "# env = get_env(win_env, False)\n",
    "\n",
    "# print(env.action_space.low, env.action_space.high)\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "env_name = \"Pendulum-v0\"\n",
    "name = f\"DPPG-{env_name}-2\"\n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "with open(\"models/2020-07-08/DPPG-Pendulum-v0-2/ddpg_agent_training.pickle\", 'rb') as f:\n",
    "    ddpg_agent = pickle.load(f)\n",
    "\n",
    "    ddpg_agent.run(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from config.config import log, logFormatter\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from gym import wrappers\n",
    "\n",
    "env_name = \"Pendulum-v0\"\n",
    "name = f\"DPPG-{env_name}-3\"\n",
    "\n",
    "folder = Path(f'models/{datetime.now().date()}/{name}/')\n",
    "folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "fh = log.FileHandler(f'models/{datetime.now().date()}/{name}/{datetime.now().date()}.log')\n",
    "fh.setFormatter(logFormatter)\n",
    "log.getLogger().addHandler(fh)\n",
    "\n",
    "\n",
    "trainer = DDPGTrainer()\n",
    "\n",
    "log.info(f\"Start DDPG training ({env_name})...\")\n",
    "\n",
    "trainer.config[\"episodes\"] = 70\n",
    "trainer.config[\"training_steps\"] = 50\n",
    "trainer.config[\"evaluation_steps\"] = 50 # To disable evaluation\n",
    "# trainer.config[\"evaluation_lim\"] = 100\n",
    "\n",
    "trainer.train(env, name=name, render=False)\n",
    "\n",
    "log.info(\"Training done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from config.config import log\n",
    "\n",
    "env = \"envs/worm_dynamic_one_agent/win/UnityEnvironment\"\n",
    "# env = \"./envs/worm_dynamic_one_agent/linux/worm_dynamic\"\n",
    "env = get_env(env, False)\n",
    "\n",
    "trainer = TD3Trainer()\n",
    "\n",
    "log.info(\"Start TD3 training (WormDomain)...\")\n",
    "\n",
    "trainer.config[\"episodes\"] = 5\n",
    "trainer.config[\"training_steps\"] = 10\n",
    "trainer.config[\"training_episodes\"] = 2\n",
    "trainer.config[\"batch_size\"] = 2\n",
    "trainer.config[\"evaluation_lim\"] = 5\n",
    "trainer.train(env, name=\"TEST-TD3-WormDomain\")\n",
    "\n",
    "log.info(\"Training done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from config.config import log\n",
    "\n",
    "env = \"envs/worm_dynamic_one_agent/win/UnityEnvironment\"\n",
    "# env = \"./envs/worm_dynamic_one_agent/linux/worm_dynamic\"\n",
    "env = get_env(env, False)\n",
    "\n",
    "trainer = DDPGTrainer()\n",
    "\n",
    "log.info(\"Start DDPG training (WormDomain)...\")\n",
    "\n",
    "trainer.config[\"episodes\"] = 3\n",
    "trainer.config[\"training_steps\"] = 10\n",
    "trainer.config[\"evaluation_lim\"] = 10\n",
    "trainer.config[\"batch_size\"] = 3\n",
    "trainer.train(env, name=\"TEST-DPPG-WormDomain\")\n",
    "\n",
    "log.info(\"Training done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "win_env = \"envs/worm_dynamic_one_agent/win/UnityEnvironment\"\n",
    "env = get_env(win_env, True)\n",
    "\n",
    "trainer = DDPGTrainer()\n",
    "\n",
    "study = trainer.start_training(env, trials=2, render=False, name=\"WormDomain-5_Default\", default=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.training_rewards_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "pd.read_csv(r\"C:\\Users\\email\\Documents\\LMU\\4_Semester\\ASP\\Project\\AlphaWorm\\dev\\models\\2020-06-26\\WormDomain-5_Default\\best_agent\\rewards.csv\").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "win_env = \"envs/worm_dynamic_one_agent/win/UnityEnvironment\"\n",
    "env = get_env(win_env, True)\n",
    "\n",
    "trainer = DDPGTrainer()\n",
    "\n",
    "study = trainer.start_training(env, trials=2, render=False, name=\"WormDomain-4_HPO_Local\", default=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/2020-06-17/WormDomain-1.pickle\", 'rb') as f:\n",
    "    best_agent = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_env = \"envs/worm_dynamic_one_agent/win/UnityEnvironment\"\n",
    "env = get_env(win_env, True)\n",
    "\n",
    "best_agent.run(env, steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/2020-06-19/WormDomain-1.pickle\", 'rb') as f:\n",
    "    best_agent = pickle.load(f)\n",
    "\n",
    "with open(\"models/2020-06-19/WormDomain-1_study.pickle\", 'rb') as f:\n",
    "    study = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "win_env = \"envs/worm_dynamic_one_agent/win/UnityEnvironment\"\n",
    "env = get_env(win_env, True)\n",
    "\n",
    "trial  = FixedTrial(study.best_params)\n",
    "trainer = DDPGTrainer()\n",
    "\n",
    "trainer.train(trial, env,  render=False, name=\"WormDomain-1x\", training_steps = 1000)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "win_env = \"envs/worm_dynamic_one_agent/win/UnityEnvironment\"\n",
    "env = get_env(None, True)\n",
    "\n",
    "trainer.ddpg_agent.run(env, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_intermediate_values(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Gym Domains\n",
    "\n",
    "@author: jhuthmacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "import os\n",
    "os.chdir(\"../\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "import gym\n",
    "from trainer.ddpg_trainer import DDPGTrainer\n",
    "from config.config import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############\n",
    "# Penduluu #\n",
    "############\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "\n",
    "trainer = DDPGTrainer()\n",
    "\n",
    "log.info(\"Start DDPG training (Pendulum)...\")\n",
    "\n",
    "trainer.config[\"training_steps\"] = 200\n",
    "trainer.config[\"episodes\"] = 50\n",
    "\n",
    "trainer.train_baseline(env, name=\"Test-File-Logging\",render=False, nb_epochs=5,\n",
    "                       nb_epoch_cycles=1, nb_rollout_steps=1,\n",
    "                       nb_train_steps=1, nb_eval_steps=1)\n",
    "\n",
    "log.info(\"Training done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open(r\"C:\\Users\\email\\Documents\\LMU\\4_Semester\\ASP\\Project\\AlphaWorm\\dev\\models\\2020-07-03\\DDPG-Pendulum-2\\ddpg_baseline_training.pickle\", \"wb+\") as f:\n",
    "    pickle.dump(trainer.ddpg_agent, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "with open(r\"C:\\Users\\email\\Documents\\LMU\\4_Semester\\ASP\\Project\\AlphaWorm\\dev\\models\\2020-07-03\\DDPG-Pendulum-2\\ddpg_baseline_training.pickle\", \"rb\") as f:\n",
    "    ddpg_agent = pickle.load(f)\n",
    "    # ddpg_agent.run(env, steps=100, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mountain Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "\n",
    "trainer = DDPGTrainer()\n",
    "\n",
    "study = trainer.start_training(env, trials=10, render=False, name=\"MountainCar-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/2020-06-17/MountainCar-1.pickle\", 'rb') as f:\n",
    "    best_agent = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_agent.run(env, steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "# Read the study from a specific run to visualize.\n",
    "with open(\"../models/2020-06-17/MountainCar_study.pickle\", 'rb') as f:\n",
    "    study = pickle.load(f)\n",
    "optuna.visualization.plot_intermediate_values(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.07.2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "# import os\n",
    "# os.chdir(\"..\")\n",
    "\n",
    "import pickle\n",
    "import optuna\n",
    "import gym\n",
    "\n",
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "\n",
    "# trainer = DDPGTrainer()\n",
    "# study = trainer.start_training(env, trials=10, render=False, name=\"MountainCar-1\")env = gym.make(\"MountainCarContinuous-v0\")\n",
    "# trainer = DDPGTrainer()\n",
    "\n",
    "##############\n",
    "# Load Study #\n",
    "##############\n",
    "with open(\"../models/2020-06-17/MountainCar_study.pickle\", 'rb') as f:\n",
    "    study = pickle.load(f)\n",
    "\n",
    "\n",
    "##############\n",
    "# Load Agent #\n",
    "##############\n",
    "with open(\"../models/2020-06-17/MountainCar.pickle\", 'rb') as f:\n",
    "    agent = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actor.to(\"cpu\")\n",
    "agent.actor.input_layer.to(\"cpu\")\n",
    "agent.critic.to(\"cpu\")\n",
    "agent.critic_target.to(\"cpu\")\n",
    "agent.actor_target.to(\"cpu\")\n",
    "agent.run(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is important to append the parent directory to the Python path to import modules from our package.\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "# For correclty referencing the directoy e.g. when you save files, you have to change the working directoy of this notebook.\n",
    "# Important: If you reimport you have to restart the kernel. Otherwise you would always go one directory above.\n",
    "import os\n",
    "os.chdir(\"../\")\n",
    "\n",
    "####################\n",
    "# Default Packages #\n",
    "####################\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "##################\n",
    "# ML/RL Packages #\n",
    "##################\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "################\n",
    "# Our Packages #\n",
    "################\n",
    "from trainer import DDPGTrainer, TD3Trainer\n",
    "from utils.mlagent_utils import get_env\n",
    "from config.config import log, logFormatter# It is important to append the parent directory to the Python path to import modules from our package.\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "# For correclty referencing the directoy e.g. when you save files, you have to change the working directoy of this notebook.\n",
    "# Important: If you reimport you have to restart the kernel. Otherwise you would always go one directory above.\n",
    "import os\n",
    "os.chdir(\"../\")\n",
    "\n",
    "####################\n",
    "# Default Packages #\n",
    "####################\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "##################\n",
    "# ML/RL Packages #\n",
    "##################\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "################\n",
    "# Our Packages #\n",
    "################\n",
    "from trainer import DDPGTrainer, TD3Trainer\n",
    "from utils.mlagent_utils import get_env\n",
    "from config.config import log, logFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# Set Training Infos #\n",
    "######################\n",
    "env_name = \"MountainCarContinuous-v0\"\n",
    "name = f\"DPPG-{env_name}\"\n",
    "\n",
    "###########################################\n",
    "# Ensure that the path exists for logging #\n",
    "###########################################\n",
    "folder = Path(f'models/{datetime.now().date()}/{name}/')\n",
    "folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Store logs directly nearby the results!\n",
    "fh = log.FileHandler(f'models/{datetime.now().date()}/{name}/{datetime.now().date()}.log')\n",
    "fh.setFormatter(logFormatter)\n",
    "log.getLogger().addHandler(fh)\n",
    "\n",
    "######################\n",
    "# Create Environment #\n",
    "######################\n",
    "env = gym.make(env_name)\n",
    "\n",
    "trainer = DDPGTrainer()\n",
    "\n",
    "log.info(f\"Start DDPG training ({env_name})...\")\n",
    "\n",
    "# If you want to customize the training.\n",
    "trainer.config[\"episodes\"] = 10\n",
    "trainer.config[\"training_steps\"] = 1000\n",
    "trainer.config[\"evaluation_steps\"] = 0 # To disable evaluation set to 0\n",
    "trainer.config[\"explore_threshold\"] = 0.1\n",
    "\n",
    "trainer.train(env, name=name, render=True)\n",
    "\n",
    "log.info(\"Training done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = trainer.ddpg_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitaspconda76abd147041447eca13278d9df449fb2",
   "display_name": "Python 3.7.7 64-bit ('asp': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}